{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "692e361b",
   "metadata": {},
   "source": [
    "# How to run a prompt plugins from file\n",
    "\n",
    "Now that you're familiar with Kernel basics, let's see how the kernel allows you to run Prompt Plugins and Prompt Functions stored on disk.\n",
    "\n",
    "A Prompt Plugin is a collection of Semantic Functions, where each function is defined with natural language that can be provided with a text file.\n",
    "\n",
    "Refer to our [glossary](https://github.com/microsoft/semantic-kernel/blob/main/docs/GLOSSARY.md) for an in-depth guide to the terms.\n",
    "\n",
    "The repository includes some examples under the [samples](https://github.com/microsoft/semantic-kernel/tree/main/samples) folder.\n",
    "\n",
    "For instance, [this](../../../prompt_template_samples/FunPlugin/Joke/skprompt.txt) is the **Joke function** part of the **FunPlugin plugin**:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3ce1efe",
   "metadata": {},
   "source": [
    "```\n",
    "WRITE EXACTLY ONE JOKE or HUMOROUS STORY ABOUT THE TOPIC BELOW.\n",
    "JOKE MUST BE:\n",
    "- G RATED\n",
    "- WORKPLACE/FAMILY SAFE\n",
    "NO SEXISM, RACISM OR OTHER BIAS/BIGOTRY.\n",
    "BE CREATIVE AND FUNNY. I WANT TO LAUGH.\n",
    "+++++\n",
    "{{$input}}\n",
    "+++++\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afdb96d6",
   "metadata": {},
   "source": [
    "Note the special **`{{$input}}`** token, which is a variable that is automatically passed when invoking the function, commonly referred to as a \"function parameter\".\n",
    "\n",
    "We'll explore later how functions can accept multiple variables, as well as invoke other functions.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3bd5134",
   "metadata": {},
   "source": [
    "In the same folder you'll notice a second [config.json](../../../prompt_template_samples/FunPlugin/Joke/config.json) file. The file is optional, and is used to set some parameters for large language models like Temperature, TopP, Stop Sequences, etc.\n",
    "\n",
    "```\n",
    "{\n",
    "  \"schema\": 1,\n",
    "  \"description\": \"Generate a funny joke\",\n",
    "  \"execution_settings\": {\n",
    "    \"default\": {\n",
    "      \"max_tokens\": 1000,\n",
    "      \"temperature\": 0.9,\n",
    "      \"top_p\": 0.0,\n",
    "      \"presence_penalty\": 0.0,\n",
    "      \"frequency_penalty\": 0.0\n",
    "    }\n",
    "  },\n",
    "  \"input_variables\": [\n",
    "    {\n",
    "      \"name\": \"input\",\n",
    "      \"description\": \"Joke subject\",\n",
    "      \"default\": \"\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"style\",\n",
    "      \"description\": \"Give a hint about the desired joke style\",\n",
    "      \"default\": \"\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "384ff07f",
   "metadata": {},
   "source": [
    "Given a prompt function defined by these files, this is how to load and use a file based prompt function.\n",
    "\n",
    "Load and configure the kernel, as usual, loading also the AI service settings defined in the [Setup notebook](00-getting-started.ipynb):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365cfc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install semantic-kernel==1.0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3903dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using service type: Service.AzureOpenAI\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the samples directory to the Python path\n",
    "samples_dir = r'C:\\Users\\sulma\\OneDrive\\Documents\\GitHub\\semantic-kernel\\python\\samples'\n",
    "sys.path.append(samples_dir)\n",
    "\n",
    "try:\n",
    "    from services import Service\n",
    "    from service_settings import ServiceSettings\n",
    "\n",
    "    service_settings = ServiceSettings(\n",
    "        global_llm_service=\"AzureOpenAI\")  # Example setting\n",
    "\n",
    "    # Select a service to use for this notebook (available services: OpenAI, AzureOpenAI, HuggingFace)\n",
    "    selectedService = (\n",
    "        Service.AzureOpenAI\n",
    "        if service_settings.global_llm_service is None\n",
    "        else Service(service_settings.global_llm_service.lower())\n",
    "    )\n",
    "    print(f\"Using service type: {selectedService}\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Ensure that 'service_settings.py' is in the correct directory and properly named.\")\n",
    "    # Additional debugging information\n",
    "    print(\"Current Python path:\")\n",
    "    for path in sys.path:\n",
    "        print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0062a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from semantic_kernel import Kernel\n",
    "\n",
    "kernel = Kernel()\n",
    "\n",
    "\n",
    "env_path = r'C:\\Users\\sulma\\OneDrive\\Documents\\GitHub\\semantic-kernel\\python\\samples\\getting_started\\.env'\n",
    "load_dotenv(env_path)\n",
    "\n",
    "service_id = None\n",
    "\n",
    "# Load the API keys and other settings from the environment variables\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai_org_id = os.getenv('OPENAI_ORG_ID')\n",
    "azure_openai_api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "azure_chat_deployment_name = os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME')\n",
    "azure_openai_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "\n",
    "\n",
    "\n",
    "if selectedService == Service.OpenAI:\n",
    "\n",
    "    from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "\n",
    "    service_id = \"default\"\n",
    "\n",
    "    kernel.add_service(\n",
    "\n",
    "\n",
    "        OpenAIChatCompletion(service_id=service_id,\n",
    "                             ai_model_id=\"gpt-3.5-turbo-1106\"),\n",
    "\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "elif selectedService == Service.AzureOpenAI:\n",
    "\n",
    "    from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "    service_id = \"default\"\n",
    "\n",
    "    kernel.add_service(\n",
    "\n",
    "\n",
    "        AzureChatCompletion(\n",
    "            service_id=service_id,\n",
    "            api_key=azure_openai_api_key,           # Use the api_key from the .env file\n",
    "            # Use the deployment_name from the .env file\n",
    "            deployment_name=azure_chat_deployment_name,\n",
    "            endpoint=azure_openai_endpoint,         # Use the endpoint from the .env file\n",
    "            env_file_path=env_path,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd5ff1f4",
   "metadata": {},
   "source": [
    "Import the plugin and all its functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56ee184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: using plugins from the samples folder\n",
    "plugins_directory = \"../../../prompt_template_samples/\"\n",
    "\n",
    "funFunctions = kernel.add_plugin(parent_directory=plugins_directory, plugin_name=\"FunPlugin\")\n",
    "\n",
    "jokeFunction = funFunctions[\"Joke\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "edd99fa0",
   "metadata": {},
   "source": [
    "How to use the plugin functions, e.g. generate a joke about \"_time travel to dinosaur age_\":\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6effe63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the T-Rex go on vacation to the future? \n",
      "Because he heard the steaks were even bigger!\n"
     ]
    }
   ],
   "source": [
    "result = await kernel.invoke(jokeFunction, input=\"travel to dinosaur age\", style=\"silly\")\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2281a1fc",
   "metadata": {},
   "source": [
    "Great, now that you know how to load a plugin from disk, let's show how you can [create and run a prompt function inline.](./03-prompt-function-inline.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
