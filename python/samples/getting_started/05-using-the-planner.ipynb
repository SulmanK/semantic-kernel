{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99a80181",
   "metadata": {},
   "source": [
    "# Introduction to the Planner\n",
    "\n",
    "The Planner is one of the fundamental concepts of the Semantic Kernel.\n",
    "\n",
    "It makes use of the collection of native and semantic functions that have been registered to the kernel and using AI, will formulate a plan to execute the given ask.\n",
    "\n",
    "From our own testing, planner works best with more powerful models like `gpt4` but sometimes you might get working plans with cheaper models like `gpt-35-turbo`. We encourage you to implement your own versions of the planner and use different models that fit your user needs.\n",
    "\n",
    "Read more about planner [here](https://aka.ms/sk/concepts/planner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eb35d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -U semantic-kernel==1.0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d548e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using service type: Service.AzureOpenAI\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the samples directory to the Python path\n",
    "samples_dir = r'C:\\Users\\sulma\\OneDrive\\Documents\\GitHub\\semantic-kernel\\python\\samples'\n",
    "sys.path.append(samples_dir)\n",
    "\n",
    "try:\n",
    "    from services import Service\n",
    "    from service_settings import ServiceSettings\n",
    "\n",
    "    service_settings = ServiceSettings(\n",
    "        global_llm_service=\"AzureOpenAI\")  # Example setting\n",
    "\n",
    "    # Select a service to use for this notebook (available services: OpenAI, AzureOpenAI, HuggingFace)\n",
    "    selectedService = (\n",
    "        Service.AzureOpenAI\n",
    "        if service_settings.global_llm_service is None\n",
    "        else Service(service_settings.global_llm_service.lower())\n",
    "    )\n",
    "    print(f\"Using service type: {selectedService}\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Ensure that 'service_settings.py' is in the correct directory and properly named.\")\n",
    "    # Additional debugging information\n",
    "    print(\"Current Python path:\")\n",
    "    for path in sys.path:\n",
    "        print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3852961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.contents.chat_history import ChatHistory  # noqa: F401\n",
    "from semantic_kernel.functions.kernel_arguments import KernelArguments  # noqa: F401\n",
    "from semantic_kernel.prompt_template.input_variable import InputVariable  # noqa: F401"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deff5675",
   "metadata": {},
   "source": [
    "Define your ASK. What do you want the Kernel to do?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "925b4ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask = \"\"\"\n",
    "Tomorrow is Valentine's day. I need to come up with a few short poems.\n",
    "She likes Shakespeare so write using his style. She speaks French so write it in French.\n",
    "Convert the text to uppercase.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61bacf1",
   "metadata": {},
   "source": [
    "### Providing plugins to the planner\n",
    "\n",
    "The planner needs to know what plugins are available to it. Here we'll give it access to the `SummarizePlugin` and `WriterPlugin` we have defined on disk. This will include many semantic functions, of which the planner will intelligently choose a subset.\n",
    "\n",
    "You can also include native functions as well. Here we'll add the TextPlugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3161dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plugin: SummarizePlugin, Function: MakeAbstractReadable\n",
      "Plugin: SummarizePlugin, Function: Notegen\n",
      "Plugin: SummarizePlugin, Function: Summarize\n",
      "Plugin: SummarizePlugin, Function: Topics\n",
      "Plugin: WriterPlugin, Function: Acronym\n",
      "Plugin: WriterPlugin, Function: AcronymGenerator\n",
      "Plugin: WriterPlugin, Function: AcronymReverse\n",
      "Plugin: WriterPlugin, Function: Brainstorm\n",
      "Plugin: WriterPlugin, Function: EmailGen\n",
      "Plugin: WriterPlugin, Function: EmailTo\n",
      "Plugin: WriterPlugin, Function: EnglishImprover\n",
      "Plugin: WriterPlugin, Function: NovelChapter\n",
      "Plugin: WriterPlugin, Function: NovelChapterWithNotes\n",
      "Plugin: WriterPlugin, Function: NovelOutline\n",
      "Plugin: WriterPlugin, Function: Rewrite\n",
      "Plugin: WriterPlugin, Function: ShortPoem\n",
      "Plugin: WriterPlugin, Function: StoryGen\n",
      "Plugin: WriterPlugin, Function: TellMeMore\n",
      "Plugin: WriterPlugin, Function: Translate\n",
      "Plugin: WriterPlugin, Function: TwoSentenceSummary\n",
      "Plugin: WriterPlugin, Function: Shakespeare\n",
      "Plugin: TextPlugin, Function: lowercase\n",
      "Plugin: TextPlugin, Function: trim\n",
      "Plugin: TextPlugin, Function: trim_end\n",
      "Plugin: TextPlugin, Function: trim_start\n",
      "Plugin: TextPlugin, Function: uppercase\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from dotenv import load_dotenv\n",
    "import semantic_kernel as sk\n",
    "import semantic_kernel.connectors.ai.open_ai as sk_oai\n",
    "from semantic_kernel.core_plugins.text_plugin import TextPlugin\n",
    "from semantic_kernel.functions.kernel_function_from_prompt import KernelFunctionFromPrompt\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "kernel = Kernel()\n",
    "\n",
    "\n",
    "env_path = r'C:\\Users\\sulma\\OneDrive\\Documents\\GitHub\\semantic-kernel\\python\\samples\\getting_started\\.env'\n",
    "load_dotenv(env_path)\n",
    "\n",
    "service_id = None\n",
    "\n",
    "# Load the API keys and other settings from the environment variables\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai_org_id = os.getenv('OPENAI_ORG_ID')\n",
    "azure_openai_api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "azure_chat_deployment_name = os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME')\n",
    "azure_openai_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "\n",
    "\n",
    "if selectedService == Service.OpenAI:\n",
    "\n",
    "    from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "\n",
    "    service_id = \"default\"\n",
    "\n",
    "    kernel.add_service(\n",
    "\n",
    "\n",
    "        OpenAIChatCompletion(service_id=service_id,\n",
    "                             ai_model_id=\"gpt-3.5-turbo-1106\"),\n",
    "\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "elif selectedService == Service.AzureOpenAI:\n",
    "\n",
    "    from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "    service_id = \"default\"\n",
    "\n",
    "    kernel.add_service(\n",
    "\n",
    "\n",
    "        AzureChatCompletion(\n",
    "            service_id=service_id,\n",
    "            api_key=azure_openai_api_key,           # Use the api_key from the .env file\n",
    "            # Use the deployment_name from the .env file\n",
    "            deployment_name=azure_chat_deployment_name,\n",
    "            endpoint=azure_openai_endpoint,         # Use the endpoint from the .env file\n",
    "            env_file_path=env_path,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        )\n",
    "    )\n",
    "\n",
    "plugins_directory = \"../../../prompt_template_samples/\"\n",
    "summarize_plugin = kernel.add_plugin(plugin_name=\"SummarizePlugin\", parent_directory=plugins_directory)\n",
    "writer_plugin = kernel.add_plugin(\n",
    "    plugin_name=\"WriterPlugin\",\n",
    "    parent_directory=plugins_directory,\n",
    ")\n",
    "text_plugin = kernel.add_plugin(plugin=TextPlugin(), plugin_name=\"TextPlugin\")\n",
    "\n",
    "shakespeare_func = KernelFunctionFromPrompt(\n",
    "    function_name=\"Shakespeare\",\n",
    "    plugin_name=\"WriterPlugin\",\n",
    "    prompt=\"\"\"\n",
    "{{$input}}\n",
    "\n",
    "Rewrite the above in the style of Shakespeare.\n",
    "\"\"\",\n",
    "    prompt_execution_settings=sk_oai.OpenAIChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        max_tokens=2000,\n",
    "        temperature=0.8,\n",
    "    ),\n",
    "    description=\"Rewrite the input in the style of Shakespeare.\",\n",
    ")\n",
    "kernel.add_function(plugin_name=\"WriterPlugin\", function=shakespeare_func)\n",
    "\n",
    "for plugin_name, plugin in kernel.plugins.items():\n",
    "    for function_name, function in plugin.functions.items():\n",
    "        print(f\"Plugin: {plugin_name}, Function: {function_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a9b6b7",
   "metadata": {},
   "source": [
    "# The Plan Object Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50f8859",
   "metadata": {},
   "source": [
    "To build more advanced planners, we need to introduce a proper Plan object that can contain all the necessary state and information needed for high quality plans.\n",
    "\n",
    "To see what that object model is, look at (https://github.com/microsoft/semantic-kernel/blob/main/python/semantic_kernel/planning/plan.py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0cb2a2",
   "metadata": {},
   "source": [
    "# Sequential Planner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c66d83",
   "metadata": {},
   "source": [
    "The sequential planner is an XML-based step-by-step planner. You can see the prompt used for it here (https://github.com/microsoft/semantic-kernel/blob/main/python/semantic_kernel/planning/sequential_planner/Plugins/SequentialPlanning/skprompt.txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2e90624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.planners import SequentialPlanner\n",
    "\n",
    "planner = SequentialPlanner(kernel, service_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d537981",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_plan = await planner.create_plan(goal=ask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f462b",
   "metadata": {},
   "source": [
    "To see the steps that the Sequential Planner will take, we can iterate over them and print their descriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7007418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The plan's steps are:\n",
      "- Turn a scenario into a short and entertaining poem using WriterPlugin-ShortPoem with parameters: {'input': \"Roses are red, violets are blue, I'm not Shakespeare, but I wrote this for you.\"}\n",
      "- Translate the input into a language of your choice using WriterPlugin-Translate with parameters: {'input': '$POEM', 'language': 'French'}\n",
      "- Rewrite the input in the style of Shakespeare using WriterPlugin-Shakespeare with parameters: {'input': '$POEM'}\n",
      "- Convert a string to uppercase using TextPlugin-uppercase with parameters: {'input': '$TRANSLATED_POEM'}\n",
      "- Convert a string to uppercase using TextPlugin-uppercase with parameters: {'input': '$SHAKESPEARE_POEM'}\n"
     ]
    }
   ],
   "source": [
    "print(\"The plan's steps are:\")\n",
    "for step in sequential_plan._steps:\n",
    "    print(\n",
    "        f\"- {step.description.replace('.', '') if step.description else 'No description'} using {step.metadata.fully_qualified_name} with parameters: {step.parameters}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db5f844",
   "metadata": {},
   "source": [
    "Let's ask the sequential planner to execute the plan.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88411884",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Something went wrong in plan step WriterPlugin.Translate:'Error occurred while invoking function Translate: (\"<class 'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion'> service failed to complete the prompt\", RateLimitError(\"Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-02-01 have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 86400 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}\"))'\n"
     ]
    },
    {
     "ename": "KernelInvokeException",
     "evalue": "('Error occurred while running plan step: (\\'Error occurred while running plan step: Error occurred while invoking function Translate: (\"<class \\\\\\'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion\\\\\\'> service failed to complete the prompt\", RateLimitError(\"Error code: 429 - {\\\\\\'error\\\\\\': {\\\\\\'code\\\\\\': \\\\\\'429\\\\\\', \\\\\\'message\\\\\\': \\\\\\'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-02-01 have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 86400 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.\\\\\\'}}\"))\\', FunctionExecutionException(\\'Error occurred while invoking function Translate: (\"<class \\\\\\'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion\\\\\\'> service failed to complete the prompt\", RateLimitError(\"Error code: 429 - {\\\\\\'error\\\\\\': {\\\\\\'code\\\\\\': \\\\\\'429\\\\\\', \\\\\\'message\\\\\\': \\\\\\'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-02-01 have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 86400 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.\\\\\\'}}\"))\\'))', KernelInvokeException('Error occurred while running plan step: Error occurred while invoking function Translate: (\"<class \\'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion\\'> service failed to complete the prompt\", RateLimitError(\"Error code: 429 - {\\'error\\': {\\'code\\': \\'429\\', \\'message\\': \\'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-02-01 have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 86400 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.\\'}}\"))', FunctionExecutionException('Error occurred while invoking function Translate: (\"<class \\'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion\\'> service failed to complete the prompt\", RateLimitError(\"Error code: 429 - {\\'error\\': {\\'code\\': \\'429\\', \\'message\\': \\'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-02-01 have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 86400 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.\\'}}\"))')))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\sulma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_handler.py:51\u001b[0m, in \u001b[0;36mOpenAIHandler._send_request\u001b[1;34m(self, request_settings)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mai_model_type \u001b[38;5;241m==\u001b[39m OpenAIModelTypes\u001b[38;5;241m.\u001b[39mCHAT:\n\u001b[1;32m---> 51\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_settings\u001b[38;5;241m.\u001b[39mprepare_settings_dict())\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\sulma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:1181\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m   1149\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   1150\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m   1151\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1179\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m   1180\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1183\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[0;32m   1184\u001b[0m             {\n\u001b[0;32m   1185\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m   1186\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m   1187\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m   1188\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m   1189\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m   1190\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m   1191\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m   1192\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m   1193\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m   1194\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m   1195\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m   1196\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m   1197\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m   1198\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m   1199\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m   1200\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m   1201\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m   1202\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m   1203\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m   1204\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m   1205\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m   1206\u001b[0m             },\n\u001b[0;32m   1207\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m   1208\u001b[0m         ),\n\u001b[0;32m   1209\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m   1210\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m   1211\u001b[0m         ),\n\u001b[0;32m   1212\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m   1213\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1214\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[0;32m   1215\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\sulma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_base_client.py:1790\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1787\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1788\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1789\u001b[0m )\n\u001b[1;32m-> 1790\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[1;32mc:\\Users\\sulma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_base_client.py:1493\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[0;32m   1484\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m   1485\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1486\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1491\u001b[0m     remaining_retries: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1492\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m-> 1493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1494\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1495\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1496\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1497\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1498\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[0;32m   1499\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\sulma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_base_client.py:1569\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[1;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[0;32m   1568\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maclose()\n\u001b[1;32m-> 1569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m   1570\u001b[0m         options,\n\u001b[0;32m   1571\u001b[0m         cast_to,\n\u001b[0;32m   1572\u001b[0m         retries,\n\u001b[0;32m   1573\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1574\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1575\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1576\u001b[0m     )\n\u001b[0;32m   1578\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1579\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sulma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_base_client.py:1615\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1613\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1615\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1616\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1617\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1618\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[0;32m   1619\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1620\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1621\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\sulma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_base_client.py:1569\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[1;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[0;32m   1568\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maclose()\n\u001b[1;32m-> 1569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m   1570\u001b[0m         options,\n\u001b[0;32m   1571\u001b[0m         cast_to,\n\u001b[0;32m   1572\u001b[0m         retries,\n\u001b[0;32m   1573\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1574\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1575\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1576\u001b[0m     )\n\u001b[0;32m   1578\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1579\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sulma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_base_client.py:1615\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1613\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1615\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1616\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1617\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1618\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[0;32m   1619\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1620\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1621\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\sulma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_base_client.py:1584\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[1;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[0;32m   1583\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1587\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1588\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1591\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1592\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-02-01 have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 86400 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mServiceResponseException\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\sulma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\semantic_kernel\\functions\\kernel_function_from_prompt.py:178\u001b[0m, in \u001b[0;36mKernelFunctionFromPrompt._invoke_internal\u001b[1;34m(self, context)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     chat_message_contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m prompt_render_result\u001b[38;5;241m.\u001b[39mai_service\u001b[38;5;241m.\u001b[39mget_chat_message_contents(\n\u001b[0;32m    179\u001b[0m         chat_history\u001b[38;5;241m=\u001b[39mchat_history,\n\u001b[0;32m    180\u001b[0m         settings\u001b[38;5;241m=\u001b[39mprompt_render_result\u001b[38;5;241m.\u001b[39mexecution_settings,\n\u001b[0;32m    181\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    182\u001b[0m     )\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\sulma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_chat_completion_base.py:111\u001b[0m, in \u001b[0;36mOpenAIChatCompletionBase.get_chat_message_contents\u001b[1;34m(self, chat_history, settings, **kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mfunction_call_behavior \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    109\u001b[0m     settings\u001b[38;5;241m.\u001b[39mfunction_call_behavior \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mfunction_call_behavior\u001b[38;5;241m.\u001b[39mauto_invoke_kernel_functions\n\u001b[0;32m    110\u001b[0m ):\n\u001b[1;32m--> 111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_chat_request(settings)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# loop for auto-invoke function calls\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sulma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_chat_completion_base.py:272\u001b[0m, in \u001b[0;36mOpenAIChatCompletionBase._send_chat_request\u001b[1;34m(self, settings)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send the chat request.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 272\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_request(request_settings\u001b[38;5;241m=\u001b[39msettings)\n\u001b[0;32m    273\u001b[0m response_metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_from_chat_response(response)\n",
      "File \u001b[1;32mc:\\Users\\sulma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_handler.py:67\u001b[0m, in \u001b[0;36mOpenAIHandler._send_request\u001b[1;34m(self, request_settings)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceResponseException(\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m service failed to complete the prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     69\u001b[0m         ex,\n\u001b[0;32m     70\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n",
      "\u001b[1;31mServiceResponseException\u001b[0m: (\"<class 'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion'> service failed to complete the prompt\", RateLimitError(\"Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-02-01 have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 86400 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}\"))",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mFunctionExecutionException\u001b[0m                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\sulma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\semantic_kernel\\planners\\plan.py:160\u001b[0m, in \u001b[0;36mPlan.invoke\u001b[1;34m(self, kernel, arguments)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function\u001b[38;5;241m.\u001b[39minvoke(kernel\u001b[38;5;241m=\u001b[39mkernel, arguments\u001b[38;5;241m=\u001b[39marguments)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\sulma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\semantic_kernel\\functions\\kernel_function.py:211\u001b[0m, in \u001b[0;36mKernelFunction.invoke\u001b[1;34m(self, kernel, arguments, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m stack \u001b[38;5;241m=\u001b[39m kernel\u001b[38;5;241m.\u001b[39mconstruct_call_stack(\n\u001b[0;32m    208\u001b[0m     filter_type\u001b[38;5;241m=\u001b[39mFilterTypes\u001b[38;5;241m.\u001b[39mFUNCTION_INVOCATION,\n\u001b[0;32m    209\u001b[0m     inner_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke_internal,\n\u001b[0;32m    210\u001b[0m )\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m stack(function_context)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function_context\u001b[38;5;241m.\u001b[39mresult\n",
      "File \u001b[1;32mc:\\Users\\sulma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\semantic_kernel\\functions\\kernel_function_from_prompt.py:184\u001b[0m, in \u001b[0;36mKernelFunctionFromPrompt._invoke_internal\u001b[1;34m(self, context)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m--> 184\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FunctionExecutionException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError occurred while invoking function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chat_message_contents:\n",
      "\u001b[1;31mFunctionExecutionException\u001b[0m: Error occurred while invoking function Translate: (\"<class 'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion'> service failed to complete the prompt\", RateLimitError(\"Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-02-01 have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 86400 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}\"))",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKernelInvokeException\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\sulma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\semantic_kernel\\planners\\plan.py:274\u001b[0m, in \u001b[0;36mPlan.invoke_next_step\u001b[1;34m(self, kernel, arguments)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m step\u001b[38;5;241m.\u001b[39minvoke(kernel, arguments)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\sulma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\semantic_kernel\\planners\\plan.py:163\u001b[0m, in \u001b[0;36mPlan.invoke\u001b[1;34m(self, kernel, arguments)\u001b[0m\n\u001b[0;32m    162\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSomething went wrong in plan step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plugin_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m KernelInvokeException(\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError occurred while running plan step: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(exc),\n\u001b[0;32m    165\u001b[0m         exc,\n\u001b[0;32m    166\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKernelInvokeException\u001b[0m: ('Error occurred while running plan step: Error occurred while invoking function Translate: (\"<class \\'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion\\'> service failed to complete the prompt\", RateLimitError(\"Error code: 429 - {\\'error\\': {\\'code\\': \\'429\\', \\'message\\': \\'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-02-01 have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 86400 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.\\'}}\"))', FunctionExecutionException('Error occurred while invoking function Translate: (\"<class \\'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion\\'> service failed to complete the prompt\", RateLimitError(\"Error code: 429 - {\\'error\\': {\\'code\\': \\'429\\', \\'message\\': \\'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-02-01 have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 86400 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.\\'}}\"))'))",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKernelInvokeException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m sequential_plan\u001b[38;5;241m.\u001b[39minvoke(kernel)\n",
      "File \u001b[1;32mc:\\Users\\sulma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\semantic_kernel\\planners\\plan.py:179\u001b[0m, in \u001b[0;36mPlan.invoke\u001b[1;34m(self, kernel, arguments)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_variables_to_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state, function_arguments)\n\u001b[0;32m    173\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvoking next step: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_step_index]\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m with arguments: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(function_arguments)\n\u001b[0;32m    178\u001b[0m )\n\u001b[1;32m--> 179\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoke_next_step(kernel, function_arguments)\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    181\u001b[0m     partial_results\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "File \u001b[1;32mc:\\Users\\sulma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\semantic_kernel\\planners\\plan.py:276\u001b[0m, in \u001b[0;36mPlan.invoke_next_step\u001b[1;34m(self, kernel, arguments)\u001b[0m\n\u001b[0;32m    274\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m step\u001b[38;5;241m.\u001b[39minvoke(kernel, arguments)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m--> 276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m KernelInvokeException(\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError occurred while running plan step: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(exc),\n\u001b[0;32m    278\u001b[0m         exc,\n\u001b[0;32m    279\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;66;03m# Update state with result\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(result)\n",
      "\u001b[1;31mKernelInvokeException\u001b[0m: ('Error occurred while running plan step: (\\'Error occurred while running plan step: Error occurred while invoking function Translate: (\"<class \\\\\\'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion\\\\\\'> service failed to complete the prompt\", RateLimitError(\"Error code: 429 - {\\\\\\'error\\\\\\': {\\\\\\'code\\\\\\': \\\\\\'429\\\\\\', \\\\\\'message\\\\\\': \\\\\\'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-02-01 have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 86400 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.\\\\\\'}}\"))\\', FunctionExecutionException(\\'Error occurred while invoking function Translate: (\"<class \\\\\\'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion\\\\\\'> service failed to complete the prompt\", RateLimitError(\"Error code: 429 - {\\\\\\'error\\\\\\': {\\\\\\'code\\\\\\': \\\\\\'429\\\\\\', \\\\\\'message\\\\\\': \\\\\\'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-02-01 have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 86400 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.\\\\\\'}}\"))\\'))', KernelInvokeException('Error occurred while running plan step: Error occurred while invoking function Translate: (\"<class \\'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion\\'> service failed to complete the prompt\", RateLimitError(\"Error code: 429 - {\\'error\\': {\\'code\\': \\'429\\', \\'message\\': \\'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-02-01 have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 86400 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.\\'}}\"))', FunctionExecutionException('Error occurred while invoking function Translate: (\"<class \\'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion\\'> service failed to complete the prompt\", RateLimitError(\"Error code: 429 - {\\'error\\': {\\'code\\': \\'429\\', \\'message\\': \\'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-02-01 have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 86400 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.\\'}}\"))')))"
     ]
    }
   ],
   "source": [
    "result = await sequential_plan.invoke(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d27aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789b651a",
   "metadata": {},
   "source": [
    "# Function Calling Stepwise Planner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4bbcc3",
   "metadata": {},
   "source": [
    "The Function Calling Stepwise Planner is based off the paper from MRKL (Modular Reasoning, Knowledge and Language) and is similar to other papers like ReACT (Reasoning and Acting in Language Models). At the core, the stepwise planner allows for the AI to form \"thoughts\" and \"observations\" and execute actions based off those to achieve a user's goal. This continues until all required functions are complete and a final output is generated.\n",
    "\n",
    "Please note that the Function Calling Stepwise Planner uses OpenAI function calling, and so it can only use either the AzureChatCompletion or the OpenAIChatCompletion service.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771bafa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "import semantic_kernel.connectors.ai.open_ai as sk_oai\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "service_id = \"default\"\n",
    "if selectedService == Service.OpenAI:\n",
    "    kernel.add_service(\n",
    "        sk_oai.OpenAIChatCompletion(service_id=service_id, ai_model_id=\"gpt-3.5-turbo-1106\"),\n",
    "    )\n",
    "elif selectedService == Service.AzureOpenAI:\n",
    "    kernel.add_service(\n",
    "        sk_oai.AzureChatCompletion(\n",
    "            service_id=service_id,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a00bde",
   "metadata": {},
   "source": [
    "Let's create a sample `EmailPlugin` that simulates handling a request to `get_email_address()` and `send_email()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb43d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from semantic_kernel.functions.kernel_function_decorator import kernel_function\n",
    "\n",
    "\n",
    "class EmailPlugin:\n",
    "    \"\"\"\n",
    "    Description: EmailPlugin provides a set of functions to send emails.\n",
    "\n",
    "    Usage:\n",
    "        kernel.import_plugin_from_object(EmailPlugin(), plugin_name=\"email\")\n",
    "\n",
    "    Examples:\n",
    "        {{email.SendEmail}} => Sends an email with the provided subject and body.\n",
    "    \"\"\"\n",
    "\n",
    "    @kernel_function(name=\"SendEmail\", description=\"Given an e-mail and message body, send an e-email\")\n",
    "    def send_email(\n",
    "        self,\n",
    "        subject: Annotated[str, \"the subject of the email\"],\n",
    "        body: Annotated[str, \"the body of the email\"],\n",
    "    ) -> Annotated[str, \"the output is a string\"]:\n",
    "        \"\"\"Sends an email with the provided subject and body.\"\"\"\n",
    "        return f\"Email sent with subject: {subject} and body: {body}\"\n",
    "\n",
    "    @kernel_function(name=\"GetEmailAddress\", description=\"Given a name, find the email address\")\n",
    "    def get_email_address(\n",
    "        self,\n",
    "        input: Annotated[str, \"the name of the person\"],\n",
    "    ):\n",
    "        email = \"\"\n",
    "        if input == \"Jane\":\n",
    "            email = \"janedoe4321@example.com\"\n",
    "        elif input == \"Paul\":\n",
    "            email = \"paulsmith5678@example.com\"\n",
    "        elif input == \"Mary\":\n",
    "            email = \"maryjones8765@example.com\"\n",
    "        else:\n",
    "            input = \"johndoe1234@example.com\"\n",
    "        return email"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feef46b",
   "metadata": {},
   "source": [
    "We'll add this new plugin to the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032d5981",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.add_plugin(plugin_name=\"EmailPlugin\", plugin=EmailPlugin())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effdf3ab",
   "metadata": {},
   "source": [
    "Let's also add a couple more plugins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe150e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.core_plugins.math_plugin import MathPlugin\n",
    "from semantic_kernel.core_plugins.time_plugin import TimePlugin\n",
    "\n",
    "kernel.add_plugin(plugin_name=\"MathPlugin\", plugin=MathPlugin())\n",
    "kernel.add_plugin(plugin_name=\"TimePlugin\", plugin=TimePlugin())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06796ade",
   "metadata": {},
   "source": [
    "We will define our FunctionCallingStepPlanner and the questions we want to ask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d08549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.planners.function_calling_stepwise_planner import (\n",
    "    FunctionCallingStepwisePlanner,\n",
    "    FunctionCallingStepwisePlannerOptions,\n",
    ")\n",
    "\n",
    "questions = [\n",
    "    \"What is the current hour number, plus 5?\",\n",
    "    \"What is 387 minus 22? Email the solution to John and Mary.\",\n",
    "    \"Write a limerick, translate it to Spanish, and send it to Jane\",\n",
    "]\n",
    "\n",
    "options = FunctionCallingStepwisePlannerOptions(\n",
    "    max_iterations=10,\n",
    "    max_tokens=4000,\n",
    ")\n",
    "\n",
    "planner = FunctionCallingStepwisePlanner(service_id=service_id, options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ed7874",
   "metadata": {},
   "source": [
    "Let's loop through the questions and invoke the planner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00c6f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    result = await planner.invoke(kernel, question)\n",
    "    print(f\"Q: {question}\\nA: {result.final_answer}\\n\")\n",
    "\n",
    "    # Uncomment the following line to view the planner's process for completing the request\n",
    "    # print(f\"Chat history: {result.chat_history}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
